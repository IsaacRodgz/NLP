{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import csv\n",
    "\n",
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for twitt in f_corpus:\n",
    "            tr_txt += [twitt.strip()]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label.strip()]        \n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./agre/mex20_train.txt\", \"./agre/mex20_train_labels.txt\")\n",
    "val_txt, val_y = get_texts_from_file(\"./agre/mex20_val.txt\", \"./agre/mex20_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5278"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y = list(map(int, tr_y))\n",
    "val_y = list(map(int, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3759, 1: 1519})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAV90lEQVR4nO3df7DldX3f8efL5YdGiUC4WrqsWZKu06AzrvQGaJ1aFIMLaV2c0XbJKKvDZNMUk5g4NhCngz/KjGmqNLSKXcvGJY0g1Rh2dA3ZIA6xLciiK7IQhisgrGzZG0GiMlKh7/5xPps57N57v2e593vuXu7zMXPmfM/7+/me8/64K6/9/jjfk6pCkqS5PG+xG5AkHf4MC0lSJ8NCktTJsJAkdTIsJEmdjljsBvpwwgkn1OrVqxe7DUlaUm6//fa/qaqJmdY9J8Ni9erV7Ny5c7HbkKQlJcl3ZlvnYShJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSp+fkN7jna/XFX1zsFvQc9sCHf3mxW5AOmXsWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpU29hkeT5Sb6W5JtJdif5QKt/Ksn9SXa1x9pWT5IrkkwluSPJqUPvtTHJve2xsa+eJUkz6/PeUE8Cr6+qHyY5Evhqki+1de+tqs8eMP4cYE17nA5cCZye5HjgUmASKOD2JNuq6rEee5ckDeltz6IGftheHtkeNccm64Gr23a3AMcmORF4I7Cjqh5tAbEDWNdX35Kkg/V6ziLJiiS7gH0M/oN/a1t1WTvUdHmSo1ttJfDQ0OZ7Wm22+oGftSnJziQ7p6enF3wukrSc9RoWVfV0Va0FTgJOS/JK4BLgHwK/CBwP/G4bnpneYo76gZ+1uaomq2pyYmJiQfqXJA2M5Wqoqvo+8BVgXVXtbYeangT+CDitDdsDrBra7CTg4TnqkqQx6fNqqIkkx7blFwBvAP66nYcgSYDzgDvbJtuAC9pVUWcAj1fVXuAG4OwkxyU5Dji71SRJY9Ln1VAnAluTrGAQStdV1ReSfDnJBIPDS7uAf93GbwfOBaaAJ4B3AlTVo0k+BNzWxn2wqh7tsW9J0gF6C4uqugN49Qz1188yvoCLZlm3BdiyoA1KkkbmN7glSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqfewiLJ85N8Lck3k+xO8oFWPznJrUnuTfKZJEe1+tHt9VRbv3rovS5p9XuSvLGvniVJM+tzz+JJ4PVV9SpgLbAuyRnA7wOXV9Ua4DHgwjb+QuCxqvoHwOVtHElOATYArwDWAR9PsqLHviVJB+gtLGrgh+3lke1RwOuBz7b6VuC8try+vaatPytJWv3aqnqyqu4HpoDT+upbknSwXs9ZJFmRZBewD9gBfBv4flU91YbsAVa25ZXAQwBt/ePAzwzXZ9hGkjQGvYZFVT1dVWuBkxjsDfzCTMPac2ZZN1v9GZJsSrIzyc7p6eln27IkaQZjuRqqqr4PfAU4Azg2yRFt1UnAw215D7AKoK1/MfDocH2GbYY/Y3NVTVbV5MTERB/TkKRlq8+roSaSHNuWXwC8AbgbuAl4Sxu2Ebi+LW9rr2nrv1xV1eob2tVSJwNrgK/11bck6WBHdA951k4EtrYrl54HXFdVX0hyF3Btkn8PfAO4qo2/CvjjJFMM9ig2AFTV7iTXAXcBTwEXVdXTPfYtSTpAb2FRVXcAr56hfh8zXM1UVT8G3jrLe10GXLbQPUqSRuM3uCVJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktSpt7BIsirJTUnuTrI7yW+1+vuTfDfJrvY4d2ibS5JMJbknyRuH6utabSrJxX31LEma2RE9vvdTwHuq6utJjgFuT7Kjrbu8qv7j8OAkpwAbgFcAfx/4yyQvb6s/BvwSsAe4Lcm2qrqrx94lSUN6C4uq2gvsbcs/SHI3sHKOTdYD11bVk8D9SaaA09q6qaq6DyDJtW2sYSFJYzKWcxZJVgOvBm5tpXcluSPJliTHtdpK4KGhzfa02mz1Az9jU5KdSXZOT08v8AwkaXnrPSySvAj4HPDuqvpb4Erg54G1DPY8PrJ/6Ayb1xz1ZxaqNlfVZFVNTkxMLEjvkqSBPs9ZkORIBkHxJ1X1pwBV9cjQ+k8CX2gv9wCrhjY/CXi4Lc9WlySNQZ9XQwW4Cri7qj46VD9xaNibgTvb8jZgQ5Kjk5wMrAG+BtwGrElycpKjGJwE39ZX35Kkg/W5Z/Ea4O3At5LsarXfA85PspbBoaQHgF8DqKrdSa5jcOL6KeCiqnoaIMm7gBuAFcCWqtrdY9+SpAP0eTXUV5n5fMP2Oba5DLhshvr2ubaTJPXLb3BLkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqNFJYJHlNkhe25bcl+WiSn+23NUnS4WLUPYsrgSeSvAr4t8B3gKt760qSdFgZNSyeqqoC1gN/WFV/CBzTX1uSpMPJqL9n8YMklwBvA16bZAVwZH9tSZIOJ6PuWfwr4Engwqr6P8BK4A9660qSdFjp3LNoexH/varesL9WVQ/iOQtJWjY69yza72A/keTFh/LGSVYluSnJ3Ul2J/mtVj8+yY4k97bn41o9Sa5IMpXkjiSnDr3Xxjb+3iQbD3GOkqR5GvWcxY+BbyXZAfxof7GqfnOObZ4C3lNVX09yDHB72/4dwI1V9eEkFwMXA78LnAOsaY/TGVyBdXqS44FLgUmg2vtsq6rHDmGekqR5GDUsvtgeI6uqvcDetvyDJHczONexHjizDdsKfIVBWKwHrm5XXd2S5NgkJ7axO6rqUYAWOOuAaw6lH0nSszdSWFTV1iQvAF5WVfcc6ockWQ28GrgVeGkLEqpqb5KXtGErgYeGNtvTarPVJUljMuo3uP8FsAv48/Z6bZJtI277IuBzwLur6m/nGjpDreaoH/g5m5LsTLJzenp6lNYkSSMa9dLZ9wOnAd8HqKpdwMldGyU5kkFQ/ElV/WkrP9IOL9Ge97X6HmDV0OYnAQ/PUX+GqtpcVZNVNTkxMTHitCRJoziUb3A/fkDtoH/dD0sS4Crg7qr66NCqbcD+K5o2AtcP1S9oV0WdATzeDlfdAJyd5Lh25dTZrSZJGpNRT3DfmeRXgBVJ1gC/Cfyvjm1eA7ydwVVUu1rt94APA9cluRB4EHhrW7cdOBeYAp4A3glQVY8m+RBwWxv3wf0nuyVJ4zFqWPwG8D4G3+K+hsG/7D801wZV9VVmPt8AcNYM4wu4aJb32gJsGbFXSdICG/VqqCcYhMX72je6X1hVP+61M0nSYWPUq6E+neSn229a7AbuSfLefluTJB0uRj3BfUq77PU8BucWXsbgfIQkaRkYNSyObJfBngdcX1U/oeNqKEnSc8eoYfEJ4H7ghcDN7SdV5/qCnSTpOWTOE9xJfmfo5eUM9ibeBnwVeF2PfUmSDiNdexbHDD1e1J4ngS8Bb+m3NUnS4WLOPYuq+sBM9Xbb8L8Eru2jKUnS4WXUcxbP0L5BPdsX7iRJzzHPKiySvB7wx4ckaZnoOsH9LQ6+RPZ4Bnd9vaCvpiRJh5eu23388wNeF/C9qvrRTIMlSc9NXSe4vzOuRiRJh69ndc5CkrS8GBaSpE6GhSSpk2EhSepkWEiSOhkWkqROvYVFki1J9iW5c6j2/iTfTbKrPc4dWndJkqkk9yR541B9XatNJbm4r34lSbPrc8/iU8C6GeqXV9Xa9tgOkOQUYAPwirbNx5OsaL/3/THgHOAU4Pw2VpI0Rl3f4H7WqurmJKtHHL4euLaqngTuTzIFnNbWTVXVfQBJrm1j71rgdqWxWX3xFxe7BT2HPfDhX+7lfRfjnMW7ktzRDlMd12orgYeGxuxptdnqB0myKcnOJDunp6f76FuSlq1xh8WVwM8Da4G9wEdafabbndcc9YOLVZurarKqJicmJhaiV0lS09thqJlU1SP7l5N8EvhCe7kHWDU09CQGd7ZljrokaUzGumeR5MShl28G9l8ptQ3YkOToJCcDa4CvAbcBa5KcnOQoBifBt42zZ0lSj3sWSa4BzgROSLIHuBQ4M8laBoeSHgB+DaCqdie5jsGJ66eAi6rq6fY+7wJuAFYAW6pqd189S5Jm1ufVUOfPUL5qjvGXAZfNUN8ObF/A1iRJh8hvcEuSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKlTb2GRZEuSfUnuHKodn2RHknvb83GtniRXJJlKckeSU4e22djG35tkY1/9SpJm1+eexaeAdQfULgZurKo1wI3tNcA5wJr22ARcCYNwAS4FTgdOAy7dHzCSpPHpLSyq6mbg0QPK64GtbXkrcN5Q/eoauAU4NsmJwBuBHVX1aFU9Buzg4ACSJPVs3OcsXlpVewHa80tafSXw0NC4Pa02W/0gSTYl2Zlk5/T09II3LknL2eFygjsz1GqO+sHFqs1VNVlVkxMTEwvanCQtd+MOi0fa4SXa875W3wOsGhp3EvDwHHVJ0hiNOyy2AfuvaNoIXD9Uv6BdFXUG8Hg7THUDcHaS49qJ7bNbTZI0Rkf09cZJrgHOBE5IsofBVU0fBq5LciHwIPDWNnw7cC4wBTwBvBOgqh5N8iHgtjbug1V14ElzSVLPeguLqjp/llVnzTC2gItmeZ8twJYFbE2SdIgOlxPckqTDmGEhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqtChhkeSBJN9KsivJzlY7PsmOJPe25+NaPUmuSDKV5I4kpy5Gz5K0nC3mnsXrqmptVU221xcDN1bVGuDG9hrgHGBNe2wCrhx7p5K0zB1Oh6HWA1vb8lbgvKH61TVwC3BskhMXo0FJWq4WKywK+IsktyfZ1Govraq9AO35Ja2+EnhoaNs9rfYMSTYl2Zlk5/T0dI+tS9Lyc8Qife5rqurhJC8BdiT56znGZoZaHVSo2gxsBpicnDxovSTp2VuUPYuqerg97wM+D5wGPLL/8FJ73teG7wFWDW1+EvDw+LqVJI09LJK8MMkx+5eBs4E7gW3AxjZsI3B9W94GXNCuijoDeHz/4SpJ0ngsxmGolwKfT7L/8z9dVX+e5DbguiQXAg8Cb23jtwPnAlPAE8A7x9+yJC1vYw+LqroPeNUM9e8BZ81QL+CiMbQmSZrF4XTprCTpMGVYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROSyYskqxLck+SqSQXL3Y/krScLImwSLIC+BhwDnAKcH6SUxa3K0laPpZEWACnAVNVdV9V/V/gWmD9IvckScvGEYvdwIhWAg8Nvd4DnD48IMkmYFN7+cMk98zj804A/mYe2y9Fy23Oy22+4JyXhfz+vOb8s7OtWCphkRlq9YwXVZuBzQvyYcnOqppciPdaKpbbnJfbfME5Lxd9zXmpHIbaA6waen0S8PAi9SJJy85SCYvbgDVJTk5yFLAB2LbIPUnSsrEkDkNV1VNJ3gXcAKwAtlTV7h4/ckEOZy0xy23Oy22+4JyXi17mnKrqHiVJWtaWymEoSdIiMiwkSZ2WbVh03T4kydFJPtPW35pk9fi7XFgjzPl3ktyV5I4kNyaZ9ZrrpWLU28QkeUuSSrLkL7McZc5J/mX7s96d5NPj7nGhjfB3+2VJbkryjfb3+9zF6HOhJNmSZF+SO2dZnyRXtP897khy6rw/tKqW3YPBSfJvAz8HHAV8EzjlgDH/BvhEW94AfGax+x7DnF8H/FRb/vXlMOc27hjgZuAWYHKx+x7Dn/Ma4BvAce31Sxa77zHMeTPw6235FOCBxe57nnN+LXAqcOcs688FvsTgO2pnALfO9zOX657FKLcPWQ9sbcufBc5KMtOXA5eKzjlX1U1V9UR7eQuD77MsZaPeJuZDwH8AfjzO5noyypx/FfhYVT0GUFX7xtzjQhtlzgX8dFt+MUv8e1pVdTPw6BxD1gNX18AtwLFJTpzPZy7XsJjp9iErZxtTVU8BjwM/M5bu+jHKnIddyOBfJktZ55yTvBpYVVVfGGdjPRrlz/nlwMuT/M8ktyRZN7bu+jHKnN8PvC3JHmA78BvjaW3RHOr/3zstie9Z9KDz9iEjjllKRp5PkrcBk8A/67Wj/s055yTPAy4H3jGuhsZglD/nIxgcijqTwd7jXyV5ZVV9v+fe+jLKnM8HPlVVH0nyj4E/bnP+f/23tygW/L9fy3XPYpTbh/zdmCRHMNh1nWu373A30i1TkrwBeB/wpqp6cky99aVrzscArwS+kuQBBsd2ty3xk9yj/t2+vqp+UlX3A/cwCI+lapQ5XwhcB1BV/xt4PoObDD5XLfgtkpZrWIxy+5BtwMa2/Bbgy9XOHC1RnXNuh2T+K4OgWOrHsaFjzlX1eFWdUFWrq2o1g/M0b6qqnYvT7oIY5e/2nzG4mIEkJzA4LHXfWLtcWKPM+UHgLIAkv8AgLKbH2uV4bQMuaFdFnQE8XlV75/OGy/IwVM1y+5AkHwR2VtU24CoGu6pTDPYoNixex/M34pz/AHgR8D/aufwHq+pNi9b0PI045+eUEed8A3B2kruAp4H3VtX3Fq/r+Rlxzu8BPpnktxkcjnnHUv7HX5JrGBxGPKGdh7kUOBKgqj7B4LzMucAU8ATwznl/5hL+30uSNCbL9TCUJOkQGBaSpE6GhSSpk2EhSepkWEiSOhkW0jwl+XtJrk3y7XYn1+1JXj7bHUGlpWhZfs9CWijt5pKfB7ZW1YZWWwu8dFEbkxaYexbS/LwO+En7IhQAVbWLoZu4JVmd5K+SfL09/kmrn5jk5iS7ktyZ5J8mWZHkU+31t9qXyKRF556FND+vBG7vGLMP+KWq+nGSNcA1DG7U+CvADVV1WZIVwE8Ba4GVVfVKgCTH9te6NDrDQurfkcB/aYennmZwLyYY3NNoS5IjgT+rql1J7gN+Lsl/Br4I/MWidCwdwMNQ0vzsBv5Rx5jfBh4BXsVgj+Io+LsfsHkt8F0G9yG7oP0g0auArwAXAf+tn7alQ2NYSPPzZeDoJL+6v5DkF4Hh3y9/MbC3/XbC2xnc7I72G+f7quqTDG5ceWq7C+zzqupzwL9j8NOZ0qLzMJQ0D1VVSd4M/KckFzP4adYHgHcPDfs48LkkbwVuAn7U6mcC703yE+CHwAUMfs3sj9oPMwFc0vskpBF411lJUicPQ0mSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnT/wfOuXkBNX5raAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(Counter(tr_y))\n",
    "plt.hist(tr_y, bins=len(set(tr_y)))\n",
    "plt.ylabel('Users');\n",
    "plt.xlabel('Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@USUARIO @USUARIO @USUARIO Q se puede esperar del maricon de closet de la Ya√±ez aun recuerdo esa ves q lo vi en zona rosa viendo quien lo levantada',\n",
       " '@USUARIO La piel nueva siempre arde un poquito los primeros d√≠as... y m√°s con este puto clima',\n",
       " 'Ustedes no se enamoran de m√≠‚Ä¶ por tontas.',\n",
       " 'Me las va a pagar esa puta gorda roba tuits...',\n",
       " '@USUARIO LA GENTE ES TONTA PORQUE NO SE DAN CUENTA QUE T√ö HACES A BATMAN AZUL',\n",
       " 'Estoy muy encabronada con las pseudo feministas por tontas e iletradas, a veces me averg√ºenza ser mujer; preferir√≠a tener un falo. #NiUnaMas',\n",
       " 'Anden putos, recuerdan el #noerapenal #Holanda fuera de #Rusia2018, esto se llama #karma ehhhhhhhh #puuuuuutos',\n",
       " 'Si no tienen chichis no traten de ense√±ar se ven muy mal y m√°s cuando son prietas.',\n",
       " 'Ojal√° asi me agarrars cuando te digo que me voy en lugar de correrme a la verga cada 5 minutos.',\n",
       " '@USUARIO @USUARIO @USUARIO @USUARIO Es solo un HDP aprovechado y que su \"Diosito Bimbo\" me perdone',\n",
       " 'La pr√≥xima vez que diga que me est√° gustando alguien, p√°rtanme la madre.',\n",
       " 'No estoy gorda, lo que pasa es que me sobra sensualidad y ya no me cabe en el cuerpo.',\n",
       " 'Desfachatada, libre, apurada, atrevida, loca y con una chispa de fiereza en la mirada. Me enamor√© en 10 minutos. Les he fallado amigos.',\n",
       " '@USUARIO @USUARIO #SiLUP sac√≥ a guardado y vali√≥ madre',\n",
       " 'Que nunca se pierda la bonita costumbre de ser nalga pronta. -La m√°s putona de las putas.',\n",
       " 'Como cuando das un buen consejo pero tu amigo est√° decidido a darse en la madre, una vez m√°s. Es admirable.',\n",
       " 'O sea qu√© pedo? Creo que me enfieste muy temprano apenas son las 9 y ya ando medio mal, putas lleguen ya',\n",
       " 'Como mandas a la verga algo de lo que dependes tanto?',\n",
       " 'Con mis acciones demuestro que aun soy muy inmaduro, pero pues.. me vale verga',\n",
       " 'Si las rolas de Mon laferte son feas, ahora en versi√≥n de banda son mucho peor',\n",
       " 'son unos putos abusivos de mierda. Iusacell NUNCA hab√≠a hecho estas porquer√≠as.',\n",
       " '#Deber√≠aSerIlegalQue no tengas esta lengua en tu verga, rigth now!!  Que no??',\n",
       " 'Chingas a tu puta y perra madre Melit√≥n pinche portero chiquitito, mal√≠simo.',\n",
       " 'prestamela para montarla tambien mientras te mama la verga',\n",
       " 'Putas canciones que se convierten en la m√°quina del tiempo.',\n",
       " 'Mam√° luchona, atenci√≥n a la bendici√≥n',\n",
       " '@USUARIO @USUARIO el t√≠tulo de cruz azul? jajajajaja que cagado eres...',\n",
       " 'Con la inocencia de un ni√±o y bajo la mano un pu√±al...',\n",
       " '#Emmys #EmmysEnTNT  @USUARIO  #StrangerThings   No porque gano esa gorda ni tiene chiste , que les pasa tienen muy mal gusto terrible...',\n",
       " 'La que es payasa, cae gorda',\n",
       " 'Que es un burro hijo de un cami√≥n lleno de putas. Ojo, solo contando el partido de hoy.',\n",
       " '@USUARIO Gente mierda son unos racistas putos anglosajones nalgas prietas!!!',\n",
       " 'Vieron cuando tapan una foto con un sticker gorda ya nos dimos cuenta que la tanga se te volo y se engancho en el ropero',\n",
       " 'Verga, dorm√≠ en la tarde para valer verga en la madrugada.',\n",
       " 'AHUEVO PUTOS COLDPLAY CERRAR√Å EL CONCIERTO  #estamosunidosmexicanos',\n",
       " 'De nada putos, nosotros si vamos al mundial @USUARIO',\n",
       " 'si estas peda y empiezas a enviarle mensaje a tu ex, bueno, ya valiste verga charlie',\n",
       " '@USUARIO @USUARIO ¬øSigue ardida por qu√© nunca la metieron al grupo de la Clika?:v',\n",
       " 'Jajajajaja gorda la que traigo entrelazada en mis piernas. Est√° riqu√≠sima',\n",
       " 'La liguilla es otro puto torneo alv paren de mamar, as√≠ entremos de 8 tengan por seguro que EL AM√âRICA se va a romper la madre!',\n",
       " '@USUARIO Que CASUALIDAD que Raul Cervantes tenga un FERRARY ?O? Sera que es un HIJO de toda su PUTA MADRE PRIISTA corrupto igual ENRRIQUE PE√ëA NIETO',\n",
       " 'Sigo necesitando pocos car√°cteres para mandarte a la verga, mi amor.',\n",
       " 'Est√∫pidas, golfas, pirujas, ¬°no tienen derechos!‚Äù, Zurita en MVS vs madres de familia | Homozapping\"',\n",
       " '@USUARIO @USUARIO @USUARIO Es #Ciencia #Gameto es Nuevo #ADN nueva persona diferente a madre #Aborto asesinato d #Veracruzano',\n",
       " 'Verga? Vagina? Van Halen? Vendetta? Viene viene? Voldemort? Vickencio? Vampiro Canadiense?',\n",
       " 'Joto yo lo digo para que tenga con que comprar sus pr√≥ximos materiales para los siguientes semestres, la tkm!',\n",
       " 'Soy mam√° soltera luchona &amp; patriota! Jajaja ‚ô•Ô∏èüá≤üáΩüéâüòç‚ò∫Ô∏è #LosAmoInfinito',\n",
       " 'La ventanita del amor se me cerr√≥ üéµ Cada que escucho esa madre me dan ganas de pachanga.',\n",
       " '\"VETE MUCH√çSIMO A LA VERGA, HIJO DE TU PERRA BOMBA MADRE\"',\n",
       " 'Wey, en TODOS LADOS est√° de la verga...pero s√≠, pinche #EstabloDeM√©xico üò™üò™üò™']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_txt[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ojal√° asi me agarrars cuando te digo que me voy en lugar de correrme a la verga cada 5 minutos.\n",
      "['Ojal√°', 'asi', 'me', 'agarrars', 'cuando', 'te', 'digo', 'que', 'me', 'voy', 'en', 'lugar', 'de', 'correrme', 'a', 'la', 'verga', 'cada', '5', 'minutos', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk import TweetTokenizer\n",
    "import re\n",
    "\n",
    "def tok(doc):\n",
    "    #tokenizer = RegexpTokenizer('(?u)\\b\\w\\w+\\b')\n",
    "    #c = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
    "    #return c.findall(doc)\n",
    "    tokenizer = TweetTokenizer()\n",
    "    return tokenizer.tokenize(doc)\n",
    "    \n",
    "print(tr_txt[8])\n",
    "print(tok(tr_txt[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "def normalize(f):\n",
    "    #f = [x.lower() for x in f]\n",
    "    #f = [x.replace(\"\\\\n\",\" \") for x in f]        \n",
    "    #f = [x.replace(\"\\\\t\",\" \") for x in f]        \n",
    "    #f = [x.replace(\"\\\\xa0\",\" \") for x in f]\n",
    "    #f = [x.replace(\"\\\\xc2\",\" \") for x in f]\n",
    "    \n",
    "    #f = [re.subn(\" [*$%&#@][*$%&#@]+\",\" xexp \", x)[0].strip() for x in f]\n",
    "    #f = [re.subn(\" [0-9]+ \",\" DD \", x)[0].strip() for x in f]\n",
    "    #f = [re.subn(\"<\\S*>\",\"\", x)[0].strip() for x in f]\n",
    "    return f\n",
    "\n",
    "def ngrams(data, labels, ntrain, mn=1, mx=1, nm=500, binary = False, donorm = False, stopwords = False, verbose = True, analyzer_char = False):\n",
    "    f = data\n",
    "    if donorm:\n",
    "        f = normalize(f)\n",
    "    \n",
    "    ftrain = f[:ntrain]\n",
    "    ftest  = f[ntrain:]\n",
    "    y_train = labels[:ntrain]\n",
    "    \n",
    "    t0 = time()\n",
    "    analyzer_type = 'word'\n",
    "    if analyzer_char:\n",
    "        analyzer_type = 'char'\n",
    "        \n",
    "    if binary:\n",
    "        vectorizer = CountVectorizer(max_n=mx,min_n=mn,binary=True)\n",
    "    elif stopwords:\n",
    "        vectorizer = TfidfVectorizer(max_n=mx,min_n=mn,stop_words='english',analyzer=analyzer_type,sublinear_tf=True)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(mn,mx),sublinear_tf=True,analyzer=analyzer_type,lowercase=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"extracting ngrams... where n is [%d,%d]\" % (mn,mx))\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(ftrain)\n",
    "    X_test = vectorizer.transform(ftest)\n",
    "    #print(ftrain[:10])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"done in %fs\" % (time() - t0), X_train.shape, X_test.shape)\n",
    "\n",
    "    y = np.array(y_train)    \n",
    "    \n",
    "    numFts = nm\n",
    "    if numFts < X_train.shape[1]:\n",
    "        t0 = time()\n",
    "        ch2 = SelectKBest(chi2, k=numFts)\n",
    "        X_train = ch2.fit_transform(X_train, y)\n",
    "        X_test = ch2.transform(X_test)\n",
    "        assert sp.issparse(X_train)        \n",
    "\n",
    "    if verbose:\n",
    "        print(\"Extracting best features by a chi-squared test.. \", X_train.shape, X_test.shape)\n",
    "    return X_train, y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting ngrams... where n is [1,1]\n",
      "done in 0.106960s (5278, 12487) (587, 12487)\n",
      "Extracting best features by a chi-squared test..  (5278, 5000) (587, 5000)\n",
      "extracting ngrams... where n is [2,2]\n",
      "done in 0.205589s (5278, 46062) (587, 46062)\n",
      "Extracting best features by a chi-squared test..  (5278, 2000) (587, 2000)\n",
      "extracting ngrams... where n is [3,3]\n",
      "done in 0.204229s (5278, 62730) (587, 62730)\n",
      "Extracting best features by a chi-squared test..  (5278, 1000) (587, 1000)\n",
      "extracting ngrams... where n is [4,4]\n",
      "done in 0.309832s (5278, 40150) (587, 40150)\n",
      "Extracting best features by a chi-squared test..  (5278, 3000) (587, 3000)\n",
      "extracting ngrams... where n is [5,5]\n",
      "done in 0.428046s (5278, 86102) (587, 86102)\n",
      "Extracting best features by a chi-squared test..  (5278, 5000) (587, 5000)\n",
      "extracting ngrams... where n is [3,3]\n",
      "done in 0.259565s (5278, 13409) (587, 13409)\n",
      "Extracting best features by a chi-squared test..  (5278, 2000) (587, 2000)\n",
      "######## Total time for feature extraction: 1.738507s (587, 27583) (587, 27583)\n"
     ]
    }
   ],
   "source": [
    "train = tr_txt\n",
    "test = val_txt\n",
    "labels = tr_y\n",
    "\n",
    "data = train + test\n",
    "n = len(data)\n",
    "ntrain = len(train)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "X_train1, y_train, X_test1 = ngrams(data, labels, ntrain, 1, 1, 5000, donorm = True, verbose = verbose)\n",
    "X_train2, y_train, X_test2 = ngrams(data, labels, ntrain, 2, 2, 2000, donorm = True, verbose = verbose)\n",
    "X_train3, y_train, X_test3 = ngrams(data, labels, ntrain, 3, 3, 1000,  donorm = True, verbose = verbose)\n",
    "X_train4, y_train, X_test4 = ngrams(data, labels, ntrain, 4, 4, 3000, donorm = True, verbose = verbose, analyzer_char = True)    \n",
    "X_train5, y_train, X_test5 = ngrams(data, labels, ntrain, 5, 5, 5000, donorm = True, verbose = verbose, analyzer_char = True)    \n",
    "X_train6, y_train, X_test6 = ngrams(data, labels, ntrain, 3, 3, 2000, donorm = True, verbose = verbose, analyzer_char = True)\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "#from numpy import genfromtxt\n",
    "#ft_tr = genfromtxt('tr.w', delimiter=' ')\n",
    "#ft_tr2=preprocessing.normalize(ft_tr, norm='l1')\n",
    "\n",
    "#from numpy import genfromtxt\n",
    "#ft_te = genfromtxt('te.w', delimiter=' ')\n",
    "#ft_te2=preprocessing.normalize(ft_te, norm='l1')\n",
    "\n",
    "TRAIN = sp.hstack([X_train1, X_train2, X_train3, X_train4, X_train5, X_train6])#, ft_tr2])\n",
    "TEST = sp.hstack([X_test1,  X_test2,  X_test3, X_test4, X_test5, X_test6])#, ft_te2])\n",
    "\n",
    "#TR_DDR = preprocessing.normalize(TR_DDR, norm='l1')\n",
    "#TE_DDR = preprocessing.normalize(TE_DDR, norm='l1')\n",
    "\n",
    "#TR_DDR = ft_tr2\n",
    "#TE_DDR = ft_te2\n",
    "if verbose:\n",
    "    print(\"######## Total time for feature extraction: %fs\" % (time() - t0), TR_DDR.shape, TE_DDR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5278, 18000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(587, 18000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[382  36]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       418\n",
      "           1       0.77      0.72      0.75       169\n",
      "\n",
      "    accuracy                           0.86       587\n",
      "   macro avg       0.83      0.82      0.82       587\n",
      "weighted avg       0.86      0.86      0.86       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=7)\n",
    "\n",
    "grid.fit(TRAIN, labels) \n",
    "\n",
    "y_pred = grid.predict(TEST)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average='macro', pos_label=None)\n",
    "\n",
    "print(confusion_matrix(val_y, y_pred))\n",
    "print(metrics.classification_report(val_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando con TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt, _ = get_texts_from_file(\"./agre/mex20_test_full.txt\", \"./agre/mex20_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting ngrams... where n is [1,1]\n",
      "done in 0.111576s (5865, 13371) (1467, 13371)\n",
      "Extracting best features by a chi-squared test..  (5865, 5000) (1467, 5000)\n",
      "extracting ngrams... where n is [2,2]\n",
      "done in 0.200366s (5865, 50293) (1467, 50293)\n",
      "Extracting best features by a chi-squared test..  (5865, 2000) (1467, 2000)\n",
      "extracting ngrams... where n is [3,3]\n",
      "done in 0.234888s (5865, 69440) (1467, 69440)\n",
      "Extracting best features by a chi-squared test..  (5865, 1000) (1467, 1000)\n",
      "extracting ngrams... where n is [4,4]\n",
      "done in 0.438125s (5865, 42253) (1467, 42253)\n",
      "Extracting best features by a chi-squared test..  (5865, 3000) (1467, 3000)\n",
      "extracting ngrams... where n is [5,5]\n",
      "done in 0.498322s (5865, 91279) (1467, 91279)\n",
      "Extracting best features by a chi-squared test..  (5865, 5000) (1467, 5000)\n",
      "extracting ngrams... where n is [3,3]\n",
      "done in 0.316037s (5865, 14043) (1467, 14043)\n",
      "Extracting best features by a chi-squared test..  (5865, 2000) (1467, 2000)\n",
      "######## Total time for feature extraction: 2.050817s (587, 27583) (587, 27583)\n"
     ]
    }
   ],
   "source": [
    "train = tr_txt + val_txt\n",
    "test = test_txt\n",
    "labels = tr_y + val_y\n",
    "\n",
    "data = train + test\n",
    "n = len(data)\n",
    "ntrain = len(train)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "X_train1, y_train, X_test1 = ngrams(data, labels, ntrain, 1, 1, 5000, donorm = True, verbose = verbose)\n",
    "X_train2, y_train, X_test2 = ngrams(data, labels, ntrain, 2, 2, 2000, donorm = True, verbose = verbose)\n",
    "X_train3, y_train, X_test3 = ngrams(data, labels, ntrain, 3, 3, 1000,  donorm = True, verbose = verbose)\n",
    "X_train4, y_train, X_test4 = ngrams(data, labels, ntrain, 4, 4, 3000, donorm = True, verbose = verbose, analyzer_char = True)    \n",
    "X_train5, y_train, X_test5 = ngrams(data, labels, ntrain, 5, 5, 5000, donorm = True, verbose = verbose, analyzer_char = True)    \n",
    "X_train6, y_train, X_test6 = ngrams(data, labels, ntrain, 3, 3, 2000, donorm = True, verbose = verbose, analyzer_char = True)\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "#from numpy import genfromtxt\n",
    "#ft_tr = genfromtxt('tr.w', delimiter=' ')\n",
    "#ft_tr2=preprocessing.normalize(ft_tr, norm='l1')\n",
    "\n",
    "#from numpy import genfromtxt\n",
    "#ft_te = genfromtxt('te.w', delimiter=' ')\n",
    "#ft_te2=preprocessing.normalize(ft_te, norm='l1')\n",
    "\n",
    "TRAIN = sp.hstack([X_train1, X_train2, X_train3, X_train4, X_train5, X_train6])#, ft_tr2])\n",
    "TEST = sp.hstack([X_test1,  X_test2,  X_test3, X_test4, X_test5, X_test6])#, ft_te2])\n",
    "\n",
    "#TR_DDR = preprocessing.normalize(TR_DDR, norm='l1')\n",
    "#TE_DDR = preprocessing.normalize(TE_DDR, norm='l1')\n",
    "\n",
    "#TR_DDR = ft_tr2\n",
    "#TE_DDR = ft_te2\n",
    "if verbose:\n",
    "    print(\"######## Total time for feature extraction: %fs\" % (time() - t0), TR_DDR.shape, TE_DDR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5865, 18000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5865"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "\n",
    "svr = svm.LinearSVC(class_weight='balanced')\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=7)\n",
    "\n",
    "grid.fit(TRAIN, labels) \n",
    "\n",
    "y_pred = grid.predict(TEST)\n",
    "\n",
    "f = open(\"pastor.csv\", \"w\")\n",
    "f.write(\"Id,Expected\\n\")\n",
    "for pred, n in zip(y_pred, range(len(y_pred))):\n",
    "    f.write(str(n) + \",\" + str(pred) + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "#p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average='macro', pos_label=None)\n",
    "\n",
    "#print(confusion_matrix(val_y, y_pred))\n",
    "#print(metrics.classification_report(val_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
