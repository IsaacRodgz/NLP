{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "\n",
    "    # Training set\n",
    "    with open('mex_train.txt', 'r') as f:\n",
    "        corpus_train = f.readlines()\n",
    "    with open('mex_train_labels.txt', 'r') as f:\n",
    "        labels_train = f.readlines()\n",
    "    labels_train = [int(lab.strip('\\n')) for lab in labels_train]\n",
    "    tweets_train = [tw.strip('\\n') for tw in corpus_train]\n",
    "\n",
    "    # Validation set\n",
    "    with open('mex_val.txt', 'r') as f:\n",
    "        corpus_val = f.readlines()\n",
    "    with open('mex_val_labels.txt', 'r') as f:\n",
    "        labels_val = f.readlines()\n",
    "    labels_val = [int(lab.strip('\\n')) for lab in labels_val]\n",
    "    tweets_val = [tw.strip('\\n') for tw in corpus_val]\n",
    "\n",
    "    # Test set\n",
    "    with open('mex_test.txt', 'r') as f:\n",
    "        corpus_test = f.readlines()\n",
    "    with open('mex_test_labels.txt', 'r') as f:\n",
    "        labels_test = f.readlines()\n",
    "    labels_test = [int(lab.strip('\\n')) for lab in labels_test]\n",
    "    tweets_test = [tw.strip('\\n') for tw in corpus_test]\n",
    "\n",
    "    return tweets_train, labels_train, tweets_val, labels_val, tweets_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5544 for training\n",
      "616 for validation\n",
      "1540 for test\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = read_data()\n",
    "print(\"{0} for training\\n{1} for validation\\n{2} for test\".format(len(X_train), len(X_val), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = 'char', ngram_range = (4, 7), min_df = 10, max_df = 1000)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5544, 27726)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [1, 1.5, 2, 2.5], 'gamma': [0.005, 0.01, .05, .1]}\n",
    "svr = SVC()\n",
    "grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/home/isaac/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=8)]: Done  80 out of  80 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=8,\n",
       "             param_grid={'C': [1, 1.5, 2, 2.5],\n",
       "                         'gamma': [0.005, 0.01, 0.05, 0.1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_macro', verbose=3)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       397\n",
      "           1       0.78      0.60      0.68       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.79      0.75      0.77       616\n",
      "weighted avg       0.80      0.80      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(X_val)\n",
    "print(metrics.classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=2.5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.005, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with countVectorizer - ngram word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "\n",
    "* SVC with C=3, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(2,2), min_df=1, max_df=2000\n",
    "* F1-score macro: 0.67\n",
    "\n",
    "Exp1:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=1, max_df=2000\n",
    "* F1-score macro: 0.72\n",
    "\n",
    "Exp2:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=5, max_df=2000\n",
    "* F1-score macro: 0.78\n",
    "\n",
    "Exp3:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=8, max_df=2000\n",
    "* F1-score macro: 0.79\n",
    "* precision macro: 0.81\n",
    "* recall macro: 0.78\n",
    "\n",
    "Exp4:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=10, max_df=2000\n",
    "* F1-score macro: 0.79\n",
    "* precision macro: 0.81\n",
    "* recall macro: 0.78\n",
    "\n",
    "Exp5:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=8, max_df=2500\n",
    "* F1-score macro: 0.79\n",
    "* precision macro: 0.82\n",
    "* recall macro: 0.78\n",
    "\n",
    "Exp6:\n",
    "\n",
    "* SVC with C=2.5, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by word(1,3), min_df=10, max_df=2500\n",
    "* F1-score macro: 0.81\n",
    "* precision macro: 0.83\n",
    "* recall macro: 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with countVectorizer - ngram char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp1:\n",
    "\n",
    "* SVC with C=2, gamma=0.05, kernel='rbf'\n",
    "* CountVectorizer with ngram by char(3,7), min_df=10, max_df=2000\n",
    "* F1-score macro: 0.79\n",
    "* precision macro: 0.81\n",
    "* recall macro: 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with Tfi-dfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
